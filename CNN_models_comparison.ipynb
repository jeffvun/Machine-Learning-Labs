{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LPGWUALWF3j6PTSIjQl1DimsfYgrtDxJ",
      "authorship_tag": "ABX9TyMHRDnBEVlQBqi9OvJjVqrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffvun/Machine-Learning-Labs/blob/main/CNN_models_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading necessary libraries**"
      ],
      "metadata": {
        "id": "go8s0gsiTO2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-keras\n",
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDN9bYkpXEvA",
        "outputId": "6d653472-4fa2-4c4f-972a-4d608ec861e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrUYgap3yiYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd8a805-97bf-4849-881f-93cd9d56d4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from vit_keras import vit, utils\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "JbraZ48RTYwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "NUM_CLASSES = 4\n"
      ],
      "metadata": {
        "id": "uDNJshJ8yotN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data directories and categories\n",
        "data_dir = '/content/drive/MyDrive/DataSet/train/ALL'\n",
        "categories = ['Benign', 'Early', 'Pre', 'Pro']\n",
        "\n",
        "# Create an empty DataFrame to store file paths and corresponding labels\n",
        "data = []\n",
        "\n",
        "# Load data by iterating through categories and collecting file paths\n",
        "for category_id, category in enumerate(categories):\n",
        "    category_dir = os.path.join(data_dir, category)\n",
        "    for filename in os.listdir(category_dir):\n",
        "        if filename.endswith('.jpg'):\n",
        "          data.append((os.path.join(category_dir, filename), category))"
      ],
      "metadata": {
        "id": "zxdqgPrW-Qud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data list into a DataFrame\n",
        "data_df = pd.DataFrame(data, columns=['FilePath', 'Category'])"
      ],
      "metadata": {
        "id": "RnF8UApbCi4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, testing, and validation sets\n",
        "train_data, test_data = train_test_split(data_df, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(f\"Number of samples in training set: {len(train_data)}\")\n",
        "print(f\"Number of samples in testing set: {len(test_data)}\")\n",
        "print(f\"Number of samples in validation set: {len(val_data)}\")\n"
      ],
      "metadata": {
        "id": "A4zPANws4ZPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85400d44-33a9-4f99-fe7a-59416535ca51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in training set: 2343\n",
            "Number of samples in testing set: 652\n",
            "Number of samples in validation set: 261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data generators\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "5iANrYLCyzRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=val_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    x_col=\"FilePath\",\n",
        "    y_col=\"Category\",\n",
        "    target_size=(IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "oTbgks8Ey51P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beb3a74-eeef-41dd-dfd2-21266b76b9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2343 validated image filenames belonging to 4 classes.\n",
            "Found 261 validated image filenames belonging to 4 classes.\n",
            "Found 652 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and Compiling Various CNN models**"
      ],
      "metadata": {
        "id": "jVNjIFfLTzVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile SVM model\n",
        "# Flatten the images for SVM\n",
        "X_svm, y_svm = [], []\n",
        "\n",
        "for i in range(len(train_generator)):\n",
        "    batch_images, batch_labels = train_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "    flattened_images = batch_images.reshape((batch_size, -1))\n",
        "    X_svm.append(flattened_images)\n",
        "    y_svm.append(np.argmax(batch_labels, axis=1))\n",
        "\n",
        "X_svm = np.vstack(X_svm)\n",
        "y_svm = np.concatenate(y_svm)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_svm, y_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "0yHKDsMhJFWK",
        "outputId": "bdc6435e-9d09-437b-a158-67b49fa30fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile CNN model\n",
        "cnn_model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*IMAGE_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "5mxq8umjLTW8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile ViT model\n",
        "vit_model = vit.vit_b16(\n",
        "    image_size=IMAGE_SIZE[0],\n",
        "    activation='softmax',\n",
        "    pretrained=False,\n",
        "    include_top=True,\n",
        "    pretrained_top=False,\n",
        "    classes=4\n",
        ")\n",
        "\n",
        "vit_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "val0a_W8LN8A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile ResNet model\n",
        "resnet_model = ResNet50(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    classes=4\n",
        ")\n",
        "\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "oj_GGC6BLcdk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and configure MobileNetV2 model\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        ")\n",
        "#Adding custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "mobilenet_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "# Compile the model\n",
        "mobilenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "NTc2h5F5y9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea435e3-90e9-4e8d-b8b3-5afea09df87b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and Evaluating Various CNN Models**"
      ],
      "metadata": {
        "id": "c8OVBSFQT_-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CNN, ViT, ResNet and mobilenet models\n",
        "\n",
        "history_cnn = cnn_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_vit = vit_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_resnet = resnet_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "history_mobilenet = mobilenet_model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n"
      ],
      "metadata": {
        "id": "V9kVChrLzOJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905df72d-3ec9-4d4c-918d-475e884f113f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 221s 3s/step - loss: 11.3463 - accuracy: 0.2638 - val_loss: 1.3177 - val_accuracy: 0.3103\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 38s 510ms/step - loss: 1.3177 - accuracy: 0.2364 - val_loss: 1.2453 - val_accuracy: 0.3410\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 40s 538ms/step - loss: 1.2636 - accuracy: 0.3525 - val_loss: 1.2002 - val_accuracy: 0.4674\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 36s 489ms/step - loss: 1.1947 - accuracy: 0.4656 - val_loss: 1.0833 - val_accuracy: 0.6360\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 38s 513ms/step - loss: 1.1487 - accuracy: 0.5395 - val_loss: 1.0808 - val_accuracy: 0.5249\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 38s 513ms/step - loss: 1.0884 - accuracy: 0.5826 - val_loss: 0.9553 - val_accuracy: 0.7011\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 39s 524ms/step - loss: 0.9784 - accuracy: 0.5958 - val_loss: 0.8549 - val_accuracy: 0.7471\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 38s 521ms/step - loss: 0.8988 - accuracy: 0.6428 - val_loss: 0.8350 - val_accuracy: 0.5824\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 39s 532ms/step - loss: 0.8325 - accuracy: 0.6658 - val_loss: 0.7469 - val_accuracy: 0.7931\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 38s 514ms/step - loss: 0.7423 - accuracy: 0.7234 - val_loss: 0.6294 - val_accuracy: 0.8238\n",
            "Epoch 1/10\n",
            "74/74 [==============================] - 140s 1s/step - loss: 2.0605 - accuracy: 0.2787 - val_loss: 1.3766 - val_accuracy: 0.2605\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 99s 1s/step - loss: 1.3945 - accuracy: 0.2778 - val_loss: 1.4391 - val_accuracy: 0.2720\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 100s 1s/step - loss: 1.4326 - accuracy: 0.2945 - val_loss: 1.3676 - val_accuracy: 0.3257\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 100s 1s/step - loss: 1.4463 - accuracy: 0.2727 - val_loss: 1.4130 - val_accuracy: 0.3257\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 98s 1s/step - loss: 1.3933 - accuracy: 0.2898 - val_loss: 1.3756 - val_accuracy: 0.2605\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 99s 1s/step - loss: 1.3572 - accuracy: 0.3508 - val_loss: 1.3110 - val_accuracy: 0.5249\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 99s 1s/step - loss: 0.9794 - accuracy: 0.5617 - val_loss: 0.8672 - val_accuracy: 0.4559\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 99s 1s/step - loss: 0.6608 - accuracy: 0.7183 - val_loss: 0.4791 - val_accuracy: 0.8008\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 100s 1s/step - loss: 0.4755 - accuracy: 0.8122 - val_loss: 0.3949 - val_accuracy: 0.8774\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 101s 1s/step - loss: 0.3940 - accuracy: 0.8429 - val_loss: 0.3892 - val_accuracy: 0.8697\n",
            "Epoch 1/10\n",
            "74/74 [==============================] - 81s 638ms/step - loss: 0.6035 - accuracy: 0.7631 - val_loss: 3.8674 - val_accuracy: 0.2605\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 46s 616ms/step - loss: 0.3431 - accuracy: 0.8903 - val_loss: 9.4026 - val_accuracy: 0.2605\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 47s 640ms/step - loss: 0.2754 - accuracy: 0.9117 - val_loss: 13.0994 - val_accuracy: 0.2605\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 46s 614ms/step - loss: 0.1924 - accuracy: 0.9394 - val_loss: 7.4255 - val_accuracy: 0.1877\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 48s 641ms/step - loss: 0.1625 - accuracy: 0.9428 - val_loss: 6.6457 - val_accuracy: 0.2414\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 46s 617ms/step - loss: 0.1279 - accuracy: 0.9595 - val_loss: 8.6747 - val_accuracy: 0.2605\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 47s 629ms/step - loss: 0.1828 - accuracy: 0.9437 - val_loss: 4.0365 - val_accuracy: 0.3563\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 47s 628ms/step - loss: 0.1325 - accuracy: 0.9560 - val_loss: 4.0648 - val_accuracy: 0.2759\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 45s 611ms/step - loss: 0.0733 - accuracy: 0.9765 - val_loss: 1.8600 - val_accuracy: 0.6513\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 46s 617ms/step - loss: 0.0638 - accuracy: 0.9787 - val_loss: 0.8361 - val_accuracy: 0.7816\n",
            "Epoch 1/10\n",
            "74/74 [==============================] - 46s 553ms/step - loss: 0.5477 - accuracy: 0.7985 - val_loss: 0.1987 - val_accuracy: 0.9157\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 40s 547ms/step - loss: 0.2332 - accuracy: 0.9129 - val_loss: 0.1454 - val_accuracy: 0.9540\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 38s 521ms/step - loss: 0.2237 - accuracy: 0.9181 - val_loss: 0.1515 - val_accuracy: 0.9387\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 38s 516ms/step - loss: 0.1748 - accuracy: 0.9343 - val_loss: 0.1051 - val_accuracy: 0.9579\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 37s 499ms/step - loss: 0.1776 - accuracy: 0.9321 - val_loss: 0.1055 - val_accuracy: 0.9655\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 38s 517ms/step - loss: 0.1434 - accuracy: 0.9479 - val_loss: 0.0945 - val_accuracy: 0.9617\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 38s 517ms/step - loss: 0.1489 - accuracy: 0.9437 - val_loss: 0.0804 - val_accuracy: 0.9770\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 43s 579ms/step - loss: 0.1415 - accuracy: 0.9420 - val_loss: 0.0673 - val_accuracy: 0.9732\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 37s 501ms/step - loss: 0.1298 - accuracy: 0.9513 - val_loss: 0.1277 - val_accuracy: 0.9579\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 37s 502ms/step - loss: 0.1260 - accuracy: 0.9552 - val_loss: 0.0684 - val_accuracy: 0.9655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SVM model\n",
        "X_test_svm, y_test_svm = utils.images_to_flat_samples(val_generator)\n",
        "svm_predictions = svm_model.predict(X_test_svm)\n",
        "svm_accuracy = accuracy_score(np.argmax(y_test_svm, axis=1), svm_predictions)\n",
        "svm_f1 = f1_score(np.argmax(y_test_svm, axis=1), svm_predictions, average='weighted')\n",
        "svm_cm = confusion_matrix(np.argmax(y_test_svm, axis=1), svm_predictions)\n"
      ],
      "metadata": {
        "id": "9GmdYyV-Ml-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "806db4ce-d97f-43ba-b924-796dbad453f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-990632acdc56>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate SVM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_to_flat_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msvm_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msvm_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvm_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'vit_keras.utils' has no attribute 'images_to_flat_samples'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate CNN model\n",
        "cnn_eval = cnn_model.evaluate(val_generator)\n",
        "cnn_f1 = f1_score(np.argmax(val_generator.labels, axis=1), np.argmax(cnn_model.predict(val_generator), axis=1), average='weighted')\n",
        "cnn_cm = confusion_matrix(np.argmax(val_generator.labels, axis=1), np.argmax(cnn_model.predict(val_generator), axis=1))\n"
      ],
      "metadata": {
        "id": "jh7JheDBMsGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ViT model\n",
        "vit_eval = vit_model.evaluate(val_generator)\n",
        "vit_f1 = f1_score(np.argmax(val_generator.labels, axis=1), np.argmax(vit_model.predict(val_generator), axis=1), average='weighted')\n",
        "vit_cm = confusion_matrix(np.argmax(val_generator.labels, axis=1), np.argmax(vit_model.predict(val_generator), axis=1))\n"
      ],
      "metadata": {
        "id": "NLsjVvbZMu8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ResNet model\n",
        "resnet_eval = resnet_model.evaluate(val_generator)\n",
        "resnet_f1 = f1_score(np.argmax(val_generator.labels, axis=1), np.argmax(resnet_model.predict(val_generator), axis=1), average='weighted')\n",
        "resnet_cm = confusion_matrix(np.argmax(val_generator.labels, axis=1), np.argmax(resnet_model.predict(val_generator), axis=1))\n"
      ],
      "metadata": {
        "id": "oWIgibXtMxV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the mobilenet model\n",
        "mobilenet_eval = mobilenet_model.evaluate(val_generator)\n",
        "mobilenet_f1 = f1_score(np.argmax(val_generator.labels, axis=1), np.argmax(mobilenet_model.predict(val_generator), axis=1), average='weighted')\n",
        "mobilenet_cm = confusion_matrix(np.argmax(val_generator.labels, axis=1), np.argmax(mobilenet_model.predict(val_generator), axis=1))\n"
      ],
      "metadata": {
        "id": "STBBglaMNmdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Models Summary**"
      ],
      "metadata": {
        "id": "tpLV3KehUJx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print and compare metrics\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"SVM F1 Score:\", svm_f1)\n",
        "print(\"SVM Confusion Matrix:\\n\", svm_cm)\n"
      ],
      "metadata": {
        "id": "KCEtPb0QO3vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN Accuracy:\", cnn_eval[1])\n",
        "print(\"CNN F1 Score:\", cnn_f1)\n",
        "print(\"CNN Confusion Matrix:\\n\", cnn_cm)\n"
      ],
      "metadata": {
        "id": "NnWGtzDKO5g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ViT Accuracy:\", vit_eval[1])\n",
        "print(\"ViT F1 Score:\", vit_f1)\n",
        "print(\"ViT Confusion Matrix:\\n\", vit_cm)\n"
      ],
      "metadata": {
        "id": "Is33yPMhO75o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ResNet Accuracy:\", mobilenet_eval[1])\n",
        "print(\"ResNet F1 Score:\", mobilenet_f1)\n",
        "print(\"ResNet Confusion Matrix:\\n\", mobilenet_cm)\n"
      ],
      "metadata": {
        "id": "KXJVqR7LO-yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MobileNet Accuracy:\", resnet_eval[1])\n",
        "print(\"MobileNet F1 Score:\", resnet_f1)\n",
        "print(\"MobileNet Confusion Matrix:\\n\", resnet_cm)\n"
      ],
      "metadata": {
        "id": "XPgVsPulSicc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['Benign', 'Early', 'Pre', 'Pro']\n",
        "\n",
        "# Plot the svm confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('SVM Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7vmiljTJEkiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cnn confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Simple CNN Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eYzOAf8_RuBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Vit confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(vit_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Vision Transformer CNN Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efbFiluzR65w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the resnet confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(resnet_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Residual Networks Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gm5j_CcOSDHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the mobilenet confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(mobilenet_cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('MobileNet Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZxHpuhuyTAMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing the models on a sample**"
      ],
      "metadata": {
        "id": "wJdXBc4PUQ65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an example image for prediction\n",
        "sample_image_path = '/content/Sample_10000.tiff'\n",
        "sample_image = keras.preprocessing.image.load_img(sample_image_path, target_size=IMAGE_SIZE)\n",
        "sample_image_array = keras.preprocessing.image.img_to_array(sample_image)\n",
        "sample_image_array = np.expand_dims(sample_image_array, axis=0)"
      ],
      "metadata": {
        "id": "usQQasZIPTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions using the trained models\n",
        "svm_pred = svm_model.predict(sample_image_array)\n",
        "cnn_pred = cnn_model.predict(sample_image_array)\n",
        "vit_pred = vit_model.predict(sample_image_array)\n",
        "resnet_pred = resnet_model.predict(sample_image_array)\n",
        "mobilenet_pred = mobilenet_model.predict(sample_image_array)\n",
        "\n",
        "# Print predictions\n",
        "print(\"SVM Predictions:\", svm_pred)\n",
        "print(\"CNN Predictions:\", cnn_pred)\n",
        "print(\"ViT Predictions:\", vit_pred)\n",
        "print(\"ResNet Predictions:\", resnet_pred)\n",
        "print(\"MobileNet Predictions:\", mobilenet_pred)"
      ],
      "metadata": {
        "id": "e65h91pnPvvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model to a directory\n",
        "svm_model.save('leukemia_classifier_svm_model')\n",
        "cnn_model.save('leukemia_classifier_cnn_model')\n",
        "vit_model.save('leukemia_classifier_vit_model')\n",
        "resnet_model.save('leukemia_classifier_resnet_model')\n",
        "mobilenet_model.save('leukemia_classifier_mobilenet_model')\n"
      ],
      "metadata": {
        "id": "L9dtBCmAzQTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Keras model architecture and weights\n",
        "svm_model.save(\"leukemia_keras_svm_model.h5\")\n",
        "cnn_model.save(\"leukemia_keras_cnn_model.h5\")\n",
        "vit_model.save(\"leukemia_keras_vit_model.h5\")\n",
        "resnet_model.save(\"leukemia_keras_resnet_model.h5\")\n",
        "mobilenet_model.save(\"leukemia_keras_mobilenet_model.h5\")\n"
      ],
      "metadata": {
        "id": "qsX2_Qd9TcbR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}